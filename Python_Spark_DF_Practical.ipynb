{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e850d1d",
   "metadata": {},
   "source": [
    "Name: Soham Mukherjee, \n",
    "Year: 3rd, \n",
    "Section: A, \n",
    "Registration no. : 2301292114"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97247e",
   "metadata": {},
   "source": [
    "Self note: \n",
    "'There's one thing to keep in mind, click over the spark_env for the python notebook that is available when the conda environment is set up.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9098b91f-2103-4a64-9fb2-223ca2430896",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 21:52:22] Logger ready.\n"
     ]
    }
   ],
   "source": [
    "# Utility: simple timestamped logger for clean outputs\n",
    "import datetime\n",
    "def log(msg):\n",
    "    now = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    print(f\"[LOG {now}] {msg}\")\n",
    "log('Logger ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7bf170-ca9f-4eb2-876f-a5ecb6c6e424",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 21:52:31] Python version: 3.11.8\n",
      "[LOG 21:52:31] JAVA_HOME=C:\\Program Files\\Java\\jdk-17\n",
      "[LOG 21:52:31] SPARK_HOME=C:\\Spark\\spark-3.5.6-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "# Check Python version and critical env vars\n",
    "import sys, os\n",
    "log(f'Python version: {sys.version.split()[0]}')\n",
    "log(f'JAVA_HOME={os.environ.get(\"JAVA_HOME\")}')\n",
    "log(f'SPARK_HOME={os.environ.get(\"SPARK_HOME\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336848e-33ce-4bb2-9e14-b63c9b84f564",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 21:52:39] findspark initialized.\n"
     ]
    }
   ],
   "source": [
    "# If Spark isn't auto-detected, set SPARK_HOME explicitly before findspark.init()\n",
    "# Example (uncomment and edit):\n",
    "# import os\n",
    "# os.environ['SPARK_HOME'] = r'C:\\\\spark'  # or '/usr/local/opt/spark'\n",
    "import findspark\n",
    "findspark.init()\n",
    "log('findspark initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef10c8a6-613c-4a8a-9036-a85925ca62d7",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 23:43:34] Spark started. Version: 3.5.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-C2GN9BI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-Practical-Notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19c7d91b090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Spark session with a small shuffle partition count for local runs\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName('Spark-Practical-Notebook')\n",
    "         .config('spark.sql.shuffle.partitions', '4')\n",
    "         .getOrCreate())\n",
    "log(f'Spark started. Version: {spark.version}')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6f5c0d-2432-4f50-822f-bd365d98c2f1",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 23:44:03] Student DataFrame created.\n",
      "+---+------+----------+-----+------+--------+\n",
      "| id|  name|attendance|score|events|projects|\n",
      "+---+------+----------+-----+------+--------+\n",
      "|  1|  Amit|        92|   88|     3|       1|\n",
      "|  2| Priya|        85|   91|     2|       2|\n",
      "|  3| Rahul|        70|   76|     1|       0|\n",
      "|  4|  Neha|        88|   82|     2|       1|\n",
      "|  5|  Ravi|        90|   95|     3|       3|\n",
      "|  6|Simran|        78|   89|     1|       1|\n",
      "+---+------+----------+-----+------+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- attendance: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- events: integer (nullable = true)\n",
      " |-- projects: integer (nullable = true)\n",
      "\n",
      "[LOG 23:44:14] Temp view `students` is ready.\n"
     ]
    }
   ],
   "source": [
    "# Define schema explicitly for clarity and type safety\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('attendance', IntegerType(), True),\n",
    "    StructField('score', IntegerType(), True),\n",
    "    StructField('events', IntegerType(), True),\n",
    "    StructField('projects', IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 'Amit',   92, 88, 3, 1),\n",
    "    (2, 'Priya',  85, 91, 2, 2),\n",
    "    (3, 'Rahul',  70, 76, 1, 0),\n",
    "    (4, 'Neha',   88, 82, 2, 1),\n",
    "    (5, 'Ravi',   90, 95, 3, 3),\n",
    "    (6, 'Simran', 78, 89, 1, 1)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "log('Student DataFrame created.')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# Create temp view for Spark SQL queries\n",
    "df.createOrReplaceTempView('students')\n",
    "log('Temp view `students` is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256043df-e64f-496e-83fa-b669bf60adee",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|  name|score|attendance|\n",
      "+------+-----+----------+\n",
      "|  Amit|   88|        92|\n",
      "| Priya|   91|        85|\n",
      "| Rahul|   76|        70|\n",
      "|  Neha|   82|        88|\n",
      "|  Ravi|   95|        90|\n",
      "|Simran|   89|        78|\n",
      "+------+-----+----------+\n",
      "\n",
      "+---+----+----------+-----+------+--------+\n",
      "| id|name|attendance|score|events|projects|\n",
      "+---+----+----------+-----+------+--------+\n",
      "|  1|Amit|        92|   88|     3|       1|\n",
      "|  4|Neha|        88|   82|     2|       1|\n",
      "|  5|Ravi|        90|   95|     3|       3|\n",
      "+---+----+----------+-----+------+--------+\n",
      "\n",
      "+---+------+----------+-----+------+--------+-------------+\n",
      "| id|  name|attendance|score|events|projects|score_boosted|\n",
      "+---+------+----------+-----+------+--------+-------------+\n",
      "|  5|  Ravi|        90|   95|     3|       3|           97|\n",
      "|  2| Priya|        85|   91|     2|       2|           93|\n",
      "|  6|Simran|        78|   89|     1|       1|           91|\n",
      "|  1|  Amit|        92|   88|     3|       1|           90|\n",
      "|  4|  Neha|        88|   82|     2|       1|           84|\n",
      "|  3| Rahul|        70|   76|     1|       0|           78|\n",
      "+---+------+----------+-----+------+--------+-------------+\n",
      "\n",
      "[LOG 23:45:07] Core data operations complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# 5.1 Simple select\n",
    "df.select('name', 'score', 'attendance').show()\n",
    "\n",
    "# 5.2 Filter rows\n",
    "df.filter(F.col('attendance') > 85).show()\n",
    "\n",
    "# 5.3 New derived column\n",
    "df2 = df.withColumn('score_boosted', F.col('score') + 2)\n",
    "df2.orderBy(F.desc('score_boosted')).show()\n",
    "log('Core data operations complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5fa2ee-de5e-46d6-b1fb-418b1d9a1219",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+-----+------+--------+\n",
      "| id|name|attendance|score|events|projects|\n",
      "+---+----+----------+-----+------+--------+\n",
      "|  1|Amit|        92|   88|     3|       1|\n",
      "+---+----+----------+-----+------+--------+\n",
      "\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Priya|\n",
      "| Ravi|\n",
      "+-----+\n",
      "\n",
      "[LOG 23:45:25] SQL queries executed.\n"
     ]
    }
   ],
   "source": [
    "# Highest attendance\n",
    "spark.sql('SELECT * FROM students ORDER BY attendance DESC LIMIT 1').show()\n",
    "# Names with score > 90\n",
    "spark.sql('SELECT name FROM students WHERE score > 90').show()\n",
    "log('SQL queries executed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d405c2-728d-4418-92f9-23ac126ffc03",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 23:45:40] Wrote DataFrame to /mnt/data/students_parquet\n",
      "[LOG 23:45:40] Read back Parquet file:\n",
      "+---+------+----------+-----+------+--------+\n",
      "| id|  name|attendance|score|events|projects|\n",
      "+---+------+----------+-----+------+--------+\n",
      "|  6|Simran|        78|   89|     1|       1|\n",
      "|  3| Rahul|        70|   76|     1|       0|\n",
      "|  2| Priya|        85|   91|     2|       2|\n",
      "|  1|  Amit|        92|   88|     3|       1|\n",
      "|  4|  Neha|        88|   82|     2|       1|\n",
      "|  5|  Ravi|        90|   95|     3|       3|\n",
      "+---+------+----------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_path = '/mnt/data/students_parquet'\n",
    "df.write.mode('overwrite').parquet(out_path)\n",
    "log(f'Wrote DataFrame to {out_path}')\n",
    "\n",
    "back = spark.read.parquet(out_path)\n",
    "log('Read back Parquet file:')\n",
    "back.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ede0071-8498-470b-9b80-f5a45594526b",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+\n",
      "|        avg_score|max_attendance|\n",
      "+-----------------+--------------+\n",
      "|86.83333333333333|            92|\n",
      "+-----------------+--------------+\n",
      "\n",
      "+--------+---+-----------------+\n",
      "|projects|  n|        avg_score|\n",
      "+--------+---+-----------------+\n",
      "|       0|  1|             76.0|\n",
      "|       1|  3|86.33333333333333|\n",
      "|       2|  1|             91.0|\n",
      "|       3|  1|             95.0|\n",
      "+--------+---+-----------------+\n",
      "\n",
      "[LOG 23:46:04] Grouping & aggregations done.\n"
     ]
    }
   ],
   "source": [
    "# Overall aggregations\n",
    "(df.groupBy()\n",
    "   .agg(F.avg('score').alias('avg_score'), F.max('attendance').alias('max_attendance'))\n",
    "   .show())\n",
    "\n",
    "# Group by projects\n",
    "(df.groupBy('projects')\n",
    "   .agg(F.count('*').alias('n'), F.avg('score').alias('avg_score'))\n",
    "   .orderBy('projects')\n",
    "   .show())\n",
    "log('Grouping & aggregations done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41f4c60-e7b1-4688-8797-54d8f323dc84",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+------+--------+-------+\n",
      "| id|  name|attendance|score|events|projects|section|\n",
      "+---+------+----------+-----+------+--------+-------+\n",
      "|  1|  Amit|        92|   88|     3|       1|      A|\n",
      "|  2| Priya|        85|   91|     2|       2|      B|\n",
      "|  3| Rahul|        70|   76|     1|       0|      A|\n",
      "|  4|  Neha|        88|   82|     2|       1|   NULL|\n",
      "|  5|  Ravi|        90|   95|     3|       3|      C|\n",
      "|  6|Simran|        78|   89|     1|       1|   NULL|\n",
      "+---+------+----------+-----+------+--------+-------+\n",
      "\n",
      "[LOG 23:46:26] Left join with sections completed.\n"
     ]
    }
   ],
   "source": [
    "# Create a small lookup to join with students\n",
    "sections = spark.createDataFrame([\n",
    "    (1, 'A'), (2, 'B'), (3, 'A'), (5, 'C')\n",
    "], ['id', 'section'])\n",
    "\n",
    "joined = df.join(sections, on='id', how='left')\n",
    "joined.show()\n",
    "log('Left join with sections completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e42cf2-fb4b-4c3f-95b6-8b219a4e4826",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "|id |scores     |profile    |\n",
      "+---+-----------+-----------+\n",
      "|1  |[88, 92, 3]|{Amit, 1}  |\n",
      "|2  |[91, 85, 2]|{Priya, 2} |\n",
      "|3  |[76, 70, 1]|{Rahul, 0} |\n",
      "|4  |[82, 88, 2]|{Neha, 1}  |\n",
      "|5  |[95, 90, 3]|{Ravi, 3}  |\n",
      "|6  |[89, 78, 1]|{Simran, 1}|\n",
      "+---+-----------+-----------+\n",
      "\n",
      "[LOG 23:46:49] Complex type demo done.\n"
     ]
    }
   ],
   "source": [
    "complex_df = (df\n",
    "    .withColumn('scores', F.array('score', 'attendance', 'events'))\n",
    "    .withColumn('profile', F.struct('name', 'projects'))\n",
    ")\n",
    "complex_df.select('id', 'scores', 'profile').show(truncate=False)\n",
    "log('Complex type demo done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fce411-ec50-4d35-be1a-97aead9dd4f3",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 23:46:55] [Mini-Lab] Skipped or failed: [PATH_NOT_FOUND] Path does not exist: file:/mnt/data/sample_transactions.csv.\n"
     ]
    }
   ],
   "source": [
    "# Example path; change to your dataset path\n",
    "path = '/mnt/data/sample_transactions.csv'  # e.g., 'C:/data/transactions.csv'\n",
    "try:\n",
    "    trans = (spark.read\n",
    "             .option('header', True)\n",
    "             .option('inferSchema', True)\n",
    "             .csv(path))\n",
    "    log(f'Loaded transactions: {trans.count()} rows')\n",
    "    trans.printSchema()\n",
    "\n",
    "    # Casting and simple KPIs\n",
    "    trans2 = (trans\n",
    "              .withColumn('amount', F.col('amount').cast('double'))\n",
    "              .withColumn('transaction_date', F.to_date('transaction_date')))\n",
    "\n",
    "    daily = (trans2.groupBy('transaction_date')\n",
    "                    .agg(F.sum('amount').alias('total_amount')))\n",
    "    daily.show(10)\n",
    "\n",
    "    top_products = (trans2.groupBy('product')\n",
    "                           .agg(F.sum('amount').alias('revenue'))\n",
    "                           .orderBy(F.desc('revenue')))\n",
    "    top_products.show(10)\n",
    "\n",
    "    # Save outputs\n",
    "    out_kpi = '/mnt/data/transaction_kpi_parquet'\n",
    "    daily.write.mode('overwrite').parquet(out_kpi)\n",
    "    log(f'Saved daily KPI to {out_kpi}')\n",
    "except Exception as e:\n",
    "    log(f'[Mini-Lab] Skipped or failed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71375a36-e4ca-42f2-84c9-df745e21de66",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 23:47:05] Spark session stopped. ✅\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "log('Spark session stopped. ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0459f3-0a39-4b13-b696-22c727e8569a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
